trainer (generalize using all 3 definitions)

{init
{weights-n
weights-n}
{bias-n
bias-n}
init}

{epoch-n
{batch-n  // mini-batch number
{fp-n
{activation-n  // activations (layer.a_), including input layer (=> data)
activation-n}
fp-n}
{bp-n
{err-output
err-output}
{err-n  // layer.del_
err-n}
{current-gradient-n  // for weights; normalized by mini-batch size.
current-gradient-n}
bp-n}
batch-n}
{update-n
{weights-gradient-n  // gradient that will be applied (given by optimizer)
weights-gradient-n}
{bias-gradient-n  // layer.b_gradients, also given by optimizer
bias-gradient-n}
{weights-n  // updated
weights-n}
{bias-n  // updated
bias-n}
update-n}
epoch-n}

in 1 epoch, n batches = n updates
in 1 batch, n fp = n bp